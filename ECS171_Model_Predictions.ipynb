{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Prepare input and target data for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# load the dataset\n",
    "def load_dataset(filename, column):\n",
    "\t# load the dataset as a pandas DataFrame\n",
    "\tdf = read_csv(filename)\n",
    "\t# split into input (X) and output (y) variables & convert to numPy array\n",
    "\tX = df.drop(column, axis = 1).values\n",
    "\ty = df[column].values\n",
    "\treturn X, y\n",
    "\n",
    "# prepare input data\n",
    "def prepare_inputs(X_train, X_test):\n",
    "\toe = OrdinalEncoder()\n",
    "\toe.fit(X_train)\n",
    "\tX_train_enc = oe.transform(X_train)\n",
    "\tX_test_enc = oe.transform(X_test)\n",
    "\t\t\n",
    "\t# scale dataset\n",
    "\tscaler = preprocessing.MinMaxScaler()\n",
    "\tX_train_rescaled = scaler.fit_transform(X_train_enc)\n",
    "\tX_test_rescaled = scaler.fit_transform(X_test_enc)\n",
    "\treturn X_train_rescaled, X_test_rescaled\n",
    "\n",
    "# prepare target\n",
    "def prepare_targets(y_train, y_test):\n",
    "\tle = LabelEncoder()\n",
    "\tle.fit(y_train)\n",
    "\ty_train_enc = le.transform(y_train)\n",
    "\ty_test_enc = le.transform(y_test)\n",
    "\treturn y_train_enc, y_test_enc\n",
    "\n",
    "# load the dataset\n",
    "X, y = load_dataset('mushrooms.csv', 'class')\n",
    "# split into train and test sets; 80/20 split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "# prepare input data\n",
    "X_train_enc, X_test_enc = prepare_inputs(X_train, X_test)\n",
    "# prepare output data\n",
    "y_train_enc, y_test_enc = prepare_targets(y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Compute Logistic Regression and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of selected features:  10 \n",
      "\n",
      "Accuracy using ALL features: 94.77\n",
      "Accuracy using Chi2: 93.35\n",
      "Accuracy using Mutual Information: 90.28\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# define how many features to select\n",
    "number_of_features = 10\n",
    "print('number of selected features: ', number_of_features, '\\n')\n",
    "\n",
    "# Feature reduction function\n",
    "def select_features(X_train, y_train, X_test, reduction_func):\n",
    " fs = SelectKBest(score_func=reduction_func, k=number_of_features)\n",
    " fs.fit(X_train, y_train)\n",
    " X_train_fs = fs.transform(X_train)\n",
    " X_test_fs = fs.transform(X_test)\n",
    " return X_train_fs, X_test_fs\n",
    "\n",
    "#### FEATURE SELECTION ####\n",
    "\n",
    "# chi2 feature selection\n",
    "X_train_fs_chi2, X_test_fs_chi2 = select_features(X_train_enc, y_train_enc, X_test_enc, chi2)\n",
    "\n",
    "# mutual information feature selection\n",
    "X_train_fs_mi, X_test_fs_mi = select_features(X_train_enc, y_train_enc, X_test_enc, mutual_info_classif)\n",
    "\n",
    "#### MODEL FITTING ####\n",
    "\n",
    "model = LogisticRegression(solver='lbfgs')\n",
    "\n",
    "# all features\n",
    "model.fit(X_train_enc, y_train_enc)\n",
    "yhat = model.predict(X_test_enc)\n",
    "accuracy = accuracy_score(y_test_enc, yhat)\n",
    "print('Accuracy using ALL features: %.2f' % (accuracy*100))\n",
    "\n",
    "# Chi2 features\n",
    "model.fit(X_train_fs_chi2, y_train_enc)\n",
    "yhat = model.predict(X_test_fs_chi2)\n",
    "accuracy = accuracy_score(y_test_enc, yhat)\n",
    "print('Accuracy using Chi2: %.2f' % (accuracy*100))\n",
    "\n",
    "# Mutual Information featuresW\n",
    "model.fit(X_train_fs_mi, y_train_enc)\n",
    "yhat = model.predict(X_test_fs_mi)\n",
    "accuracy = accuracy_score(y_test_enc, yhat)\n",
    "print('Accuracy using Mutual Information: %.2f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since feature reduction always decreases accuracy comapred to using all features, we have decided to only train further models using all features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Compute accuracy of Neural Network using All Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3a. Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:  {'hidden_layer_sizes': (100,), 'learning_rate_init': 0.1, 'max_iter': 200}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Performing some hyperparameter tuning (it can take aorund 3 minutes)\n",
    "max_iterations = [200,500,800,1000]\n",
    "hidden_layer_siz = [(100,), (50,), (100, 25), (25, 50), (70, 20)]\n",
    "learning_rates = 0.1 * np.arange(1, 4)\n",
    "param_grid = dict(learning_rate_init = learning_rates, hidden_layer_sizes = hidden_layer_siz, max_iter = max_iterations)\n",
    "\n",
    "#set model\n",
    "mlp = MLPClassifier(solver = 'adam', random_state = 42, activation = 'logistic', learning_rate_init = 0.3, batch_size = 100, hidden_layer_sizes = (12, 3), max_iter = 500)\n",
    "\n",
    "# For Grid Search\n",
    "grid = GridSearchCV(estimator = mlp, param_grid = param_grid)\n",
    "\n",
    "# Train the model with grid search\n",
    "grid_result = grid.fit(X_train_enc, y_train_enc)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best parameters: \", grid.best_params_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3b. Fitting the model with the tuned hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.00\n",
      "Classification Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       820\n",
      "           1       1.00      1.00      1.00       805\n",
      "\n",
      "    accuracy                           1.00      1625\n",
      "   macro avg       1.00      1.00      1.00      1625\n",
      "weighted avg       1.00      1.00      1.00      1625\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#set model with the best hyperparameters that we just computed\n",
    "mlp = MLPClassifier(solver = 'adam', random_state = 42, activation = 'logistic', learning_rate_init = grid_result.best_params_[\"learning_rate_init\"], batch_size = 100, hidden_layer_sizes = grid_result.best_params_[\"hidden_layer_sizes\"], max_iter = grid_result.best_params_[\"max_iter\"])\n",
    "\n",
    "mlp.fit(X_train_enc, y_train_enc)\n",
    "pred = mlp.predict(X_test_enc)\n",
    "\n",
    "accuracy = accuracy_score(y_test_enc, pred)\n",
    "print('Accuracy: %.2f' % (accuracy*100))\n",
    "print(\"Classification Report : \")\n",
    "print(classification_report(y_test_enc, pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3c. k-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE\n",
      "[0.         0.00613497 0.         0.01840491 0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "Average MSE =  0.00245398773006135\n"
     ]
    }
   ],
   "source": [
    "# Using sklearn function cross_validate()\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "CV = cross_validate(mlp, X_test_enc, y_test_enc, cv=10, scoring=['accuracy', 'neg_mean_squared_error'])\n",
    "\n",
    "print('MSE')\n",
    "print(-1*CV['test_neg_mean_squared_error'])\n",
    "print('Average MSE = ', sum(-1 * CV['test_neg_mean_squared_error']) / len(CV['test_neg_mean_squared_error']))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
