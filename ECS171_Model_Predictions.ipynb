{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Prepare input and target data for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# load the dataset\n",
    "def load_dataset(filename, column):\n",
    "\t# load the dataset as a pandas DataFrame\n",
    "\tdf = read_csv(filename)\n",
    "\t# split into input (X) and output (y) variables & convert to numPy array\n",
    "\tX = df.drop(column, axis = 1).values\n",
    "\ty = df[column].values\n",
    "\treturn X, y\n",
    "\n",
    "# prepare input data\n",
    "def prepare_inputs(X_train, X_test):\n",
    "\toe = OrdinalEncoder()\n",
    "\toe.fit(X_train)\n",
    "\tX_train_enc = oe.transform(X_train)\n",
    "\tX_test_enc = oe.transform(X_test)\n",
    "\t\t\n",
    "\t# scale dataset\n",
    "\tscaler = preprocessing.MinMaxScaler()\n",
    "\tX_train_rescaled = scaler.fit_transform(X_train_enc)\n",
    "\tX_test_rescaled = scaler.fit_transform(X_test_enc)\n",
    "\treturn X_train_rescaled, X_test_rescaled\n",
    "\n",
    "# prepare target\n",
    "def prepare_targets(y_train, y_test):\n",
    "\tle = LabelEncoder()\n",
    "\tle.fit(y_train)\n",
    "\ty_train_enc = le.transform(y_train)\n",
    "\ty_test_enc = le.transform(y_test)\n",
    "\treturn y_train_enc, y_test_enc\n",
    "\n",
    "# load the dataset\n",
    "X, y = load_dataset('mushrooms.csv', 'class')\n",
    "# split into train and test sets; 80/20 split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "# prepare input data\n",
    "X_train_enc, X_test_enc = prepare_inputs(X_train, X_test)\n",
    "# prepare output data\n",
    "y_train_enc, y_test_enc = prepare_targets(y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Compute Logistic Regression and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of selected features:  10 \n",
      "\n",
      "Accuracy using ALL features: 94.77\n",
      "Accuracy using Chi2: 93.35\n",
      "Accuracy using Mutual Information: 91.75\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# define how many features to select\n",
    "number_of_features = 10\n",
    "print('number of selected features: ', number_of_features, '\\n')\n",
    "\n",
    "# Feature reduction function\n",
    "def select_features(X_train, y_train, X_test, reduction_func):\n",
    " fs = SelectKBest(score_func=reduction_func, k=number_of_features)\n",
    " fs.fit(X_train, y_train)\n",
    " X_train_fs = fs.transform(X_train)\n",
    " X_test_fs = fs.transform(X_test)\n",
    " return X_train_fs, X_test_fs\n",
    "\n",
    "#### FEATURE SELECTION ####\n",
    "\n",
    "# chi2 feature selection\n",
    "X_train_fs_chi2, X_test_fs_chi2 = select_features(X_train_enc, y_train_enc, X_test_enc, chi2)\n",
    "\n",
    "# mutual information feature selection\n",
    "X_train_fs_mi, X_test_fs_mi = select_features(X_train_enc, y_train_enc, X_test_enc, mutual_info_classif)\n",
    "\n",
    "#### MODEL FITTING ####\n",
    "\n",
    "model = LogisticRegression(solver='lbfgs')\n",
    "\n",
    "# all features\n",
    "model.fit(X_train_enc, y_train_enc)\n",
    "yhat = model.predict(X_test_enc)\n",
    "accuracy = accuracy_score(y_test_enc, yhat)\n",
    "print('Accuracy using ALL features: %.2f' % (accuracy*100))\n",
    "\n",
    "# Chi2 features\n",
    "model.fit(X_train_fs_chi2, y_train_enc)\n",
    "yhat = model.predict(X_test_fs_chi2)\n",
    "accuracy = accuracy_score(y_test_enc, yhat)\n",
    "print('Accuracy using Chi2: %.2f' % (accuracy*100))\n",
    "\n",
    "# Mutual Information featuresW\n",
    "model.fit(X_train_fs_mi, y_train_enc)\n",
    "yhat = model.predict(X_test_fs_mi)\n",
    "accuracy = accuracy_score(y_test_enc, yhat)\n",
    "print('Accuracy using Mutual Information: %.2f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since feature reduction always decreases accuracy comapred to using all features, we have decided to only train further models using all features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Compute accuracy of Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Naive Bayes: 91.63\n",
      "\n",
      "Confusion matrix raw: \n",
      "[[753  67]\n",
      " [ 69 736]]\n",
      "\n",
      "Classification Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.92      0.92       820\n",
      "           1       0.92      0.91      0.92       805\n",
      "\n",
      "    accuracy                           0.92      1625\n",
      "   macro avg       0.92      0.92      0.92      1625\n",
      "weighted avg       0.92      0.92      0.92      1625\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "model = GaussianNB()\n",
    "model.fit(X_train_enc, y_train_enc)\n",
    "pred = model.predict(X_test_enc)\n",
    "\n",
    "# 3. evaluate model\n",
    "accuracy = accuracy_score(y_test_enc, pred)\n",
    "print('Accuracy of Naive Bayes: %.2f' % (accuracy*100))\n",
    "\n",
    "# 3b. confusion matrix\n",
    "cm = confusion_matrix(y_test_enc, pred)  # actual, pred\n",
    "print(\"\\nConfusion matrix raw: \")\n",
    "print(cm)\n",
    "\n",
    "print(\"\\nClassification Report : \")\n",
    "print(classification_report(y_test_enc, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Compute accuracy of Neural Network using All Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4a. Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:  {'hidden_layer_sizes': (100,), 'learning_rate_init': 0.1, 'max_iter': 200}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Performing some hyperparameter tuning (it can take aorund 3 minutes)\n",
    "max_iterations = [200,500,800,1000]\n",
    "hidden_layer_siz = [(100,), (50,), (100, 25), (25, 7), (70, 20), (12, 10), (5, 20)]\n",
    "learning_rates = 0.1 * np.arange(1, 4)\n",
    "param_grid = dict(learning_rate_init = learning_rates, hidden_layer_sizes = hidden_layer_siz, max_iter = max_iterations)\n",
    "\n",
    "#set model\n",
    "mlp = MLPClassifier(solver = 'adam', random_state = 42, activation = 'logistic', learning_rate_init = 0.3, batch_size = 100, hidden_layer_sizes = (12, 3), max_iter = 500)\n",
    "\n",
    "# For Grid Search\n",
    "grid = GridSearchCV(estimator = mlp, param_grid = param_grid)\n",
    "\n",
    "# Train the model with grid search\n",
    "grid_result = grid.fit(X_train_enc, y_train_enc)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best parameters: \", grid.best_params_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4b. Fitting the model with the tuned hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.00\n",
      "\n",
      "Confusion Matrix for each label : \n",
      "[[[805   0]\n",
      "  [  0 820]]\n",
      "\n",
      " [[820   0]\n",
      "  [  0 805]]]\n",
      "\n",
      "Classification Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       820\n",
      "           1       1.00      1.00      1.00       805\n",
      "\n",
      "    accuracy                           1.00      1625\n",
      "   macro avg       1.00      1.00      1.00      1625\n",
      "weighted avg       1.00      1.00      1.00      1625\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, multilabel_confusion_matrix\n",
    "\n",
    "#set model with the best hyperparameters that we just computed\n",
    "mlp = MLPClassifier(solver = 'adam', random_state = 42, activation = 'logistic', learning_rate_init = grid_result.best_params_[\"learning_rate_init\"], batch_size = 100, hidden_layer_sizes = grid_result.best_params_[\"hidden_layer_sizes\"], max_iter = grid_result.best_params_[\"max_iter\"])\n",
    "\n",
    "mlp.fit(X_train_enc, y_train_enc)\n",
    "pred = mlp.predict(X_test_enc)\n",
    "\n",
    "accuracy = accuracy_score(y_test_enc, pred)\n",
    "print('Accuracy: %.2f' % (accuracy*100))\n",
    "\n",
    "print(\"\\nConfusion Matrix for each label : \")\n",
    "print(multilabel_confusion_matrix(y_test_enc, pred))\n",
    "\n",
    "print(\"\\nClassification Report : \")\n",
    "print(classification_report(y_test_enc, pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4c. k-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy\n",
      "[1.         0.99386503 1.         0.98159509 1.         1.\n",
      " 1.         1.         1.         1.        ]\n",
      "MSE\n",
      "[0.         0.00613497 0.         0.01840491 0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "Average Accuracy =  0.9975460122699387\n",
      "Average MSE =  0.00245398773006135\n"
     ]
    }
   ],
   "source": [
    "# Using sklearn function cross_validate()\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "CV = cross_validate(mlp, X_test_enc, y_test_enc, cv=10, scoring=['accuracy', 'neg_mean_squared_error'])\n",
    "print('Accuracy')\n",
    "print(CV['test_accuracy'])\n",
    "print('MSE')\n",
    "print(-1*CV['test_neg_mean_squared_error'])\n",
    "\n",
    "print('Average Accuracy = ', sum(CV['test_accuracy']) / len(CV['test_accuracy']))\n",
    "print('Average MSE = ', sum(-1 * CV['test_neg_mean_squared_error']) / len(CV['test_neg_mean_squared_error']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy for batch  1  :  1.0\n",
      "Mean Square Error for batch  1  :  0.0\n",
      "\n",
      "Accuracy for batch  2  :  1.0\n",
      "Mean Square Error for batch  2  :  0.0\n",
      "\n",
      "Accuracy for batch  3  :  1.0\n",
      "Mean Square Error for batch  3  :  0.0\n",
      "\n",
      "Accuracy for batch  4  :  1.0\n",
      "Mean Square Error for batch  4  :  0.0\n",
      "\n",
      "Accuracy for batch  5  :  1.0\n",
      "Mean Square Error for batch  5  :  0.0\n",
      "\n",
      "Accuracy for batch  6  :  0.9987684729064039\n",
      "Mean Square Error for batch  6  :  0.0012315270935960591\n",
      "\n",
      "Accuracy for batch  7  :  0.9926108374384236\n",
      "Mean Square Error for batch  7  :  0.007389162561576354\n",
      "\n",
      "Accuracy for batch  8  :  0.9987684729064039\n",
      "Mean Square Error for batch  8  :  0.0012315270935960591\n",
      "\n",
      "Accuracy for batch  9  :  1.0\n",
      "Mean Square Error for batch  9  :  0.0\n",
      "\n",
      "Accuracy for batch  10  :  0.979064039408867\n",
      "Mean Square Error for batch  10  :  0.020935960591133004\n",
      "\n",
      "Average Accuracy =  0.9969211822660098\n",
      "Average MSE =  0.0030788177339901475\n"
     ]
    }
   ],
   "source": [
    "# To find list of accuracy and MSE values\n",
    "# Without using the sklearn function cross_validate()\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Pre-process data for K-Folding\n",
    "df = read_csv('mushrooms.csv')\n",
    "X = df.drop('class', axis = 1)\n",
    "y = df['class']\n",
    "\n",
    "oe = OrdinalEncoder()\n",
    "oe.fit(X)\n",
    "X = oe.transform(X)\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(y)\n",
    "y = le.transform(y)\n",
    "\n",
    "# step 1: randomize the dataset and create k equal size partitions\n",
    "n_splits=10\n",
    "kf = KFold(n_splits=n_splits)\n",
    "\n",
    "acc = 0\n",
    "mse = 0\n",
    "\n",
    "i = 0 #keep track of batch number\n",
    "\n",
    "# step 5: iterate k times with a different testing subset\n",
    "for train_indices, test_indices in kf.split(X):\n",
    "\n",
    "    # step 2-3: use k-1/k^th partition for the training/testing model\n",
    "    start_train, stop_train = train_indices[0], train_indices[-1]+1\n",
    "    start_test, stop_test = test_indices[0], test_indices[-1]+1\n",
    "    \n",
    "    # perform the training similar to Q1\n",
    "    #this was based on the requirements in Q1\n",
    "    mlp = MLPClassifier(solver = 'sgd', random_state = 42, activation = 'logistic', learning_rate_init = 0.3, batch_size = 100, hidden_layer_sizes = (12, 3), max_iter = 500)\n",
    "    mlp.fit(X[start_train:stop_train], y[start_train:stop_train])\n",
    "    pred = mlp.predict(X[start_test:stop_test])\n",
    "    \n",
    "    # step 4: record the evaluating scores\n",
    "    i+=1\n",
    "    acc += accuracy_score(y[start_test:stop_test], pred)\n",
    "    mse += mean_squared_error(y[start_test:stop_test], pred)\n",
    "    \n",
    "    print(\"\\nAccuracy for batch \", i, \" : \", accuracy_score(y[start_test:stop_test], pred))\n",
    "    print(\"Mean Square Error for batch \", i, \" : \", mean_squared_error(y[start_test:stop_test], pred))\n",
    "\n",
    "# step 6: find the average and select the batch with highest evaluation scores\n",
    "print('\\nAverage Accuracy = ', acc / n_splits)\n",
    "print('Average MSE = ', mse / n_splits)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
